# Neural Networks from Scratch: Backpropagation & Gradient Descent
A manual implementation of neural networks, backpropagation, and gradient descent following Andrej Karpathy's "Zero to Hero" neural network series. This project demonstrates a deep understanding of the fundamental mathematics and mechanics behind neural networks by building everything from the ground up.

### This repository contains a from-scratch implementation of:
* **Multi-Layer Perceptron (MLP)** class with manual forward and backward passes
* **Backpropagation** algorithm implemented step-by-step
* **Gradient descent** optimization without using any ML frameworks
* **Automatic differentiation** engine for computing gradients

