# Neural Networks: Backpropagation & Gradient Descent
A manual implementation of neural networks, backpropagation, and gradient descent following Andrej Karpathy's "Zero to Hero" neural network series. This project demonstrates a deep understanding of the fundamental mathematics and mechanics behind neural networks by building everything from the ground up, with reference to **micrograd**, an autograd engine designed and implemented by Andrej Karpathy.

### This repository contains a from-scratch implementation of:
* **Multi-Layer Perceptron (MLP)** class with manual forward and backward passes
* **Backpropagation** algorithm implemented step-by-step
* **Gradient descent** optimization without using any ML frameworks
* **Automatic differentiation** engine for computing gradients

**References:**
* https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
* https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd
